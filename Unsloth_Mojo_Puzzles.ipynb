{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daver987/unsloth-mojo/blob/main/Unsloth_Mojo_Puzzles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsloth `nf4` to `fp16/bf16` Challenge Mapped to Mojo"
      ],
      "metadata": {
        "id": "KmcSP-PxTAKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_rx9FYMOc2T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EI_d4FLkR51i"
      },
      "outputs": [],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-06up4cv2gr",
        "outputId": "e7da9ed9-fd37-4c4f-b3b0-39e3fdcec55e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-4c8a1b8c-de69-7d1e-2b39-17ce88820afa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert `nf4` to Triton. (Original Challenge)"
      ],
      "metadata": {
        "id": "GnthZ-u_TMLc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoE2DGRZG2Ng"
      },
      "source": [
        "<a name=\"NF4\"></a>\n",
        "\n",
        "\n",
        "\n",
        "1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n",
        "2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n",
        "3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n",
        "4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n",
        "5. Use `test_dequantize_function` to test your implementation.\n",
        "6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n",
        "7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WKQ9hdqNOXpe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96bc557-900e-44f5-9b9c-9b36419111f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1294683329.py:3: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth.kernels.utils import fast_dequantize\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EiO1cu2YKB"
      },
      "source": [
        "For example, we can test our implementation via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM8q3rDX1XfZ",
        "outputId": "bde83ff9-ee20-437d-c438-4bab97e6024b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.713529586791992"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from unsloth.kernels.utils import fast_dequantize\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETwlex22lMN"
      },
      "source": [
        "The elapsed time for our implementation over 1000 trials is 5.38 seconds or so.\n",
        "\n",
        "PEFT also has one, which should be mostly identical to Unsloth's version, albeit slightly slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5RShLO1h-Y",
        "outputId": "7b67ce9f-9f93-412c-bf41-73f579c4212a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.664241313934326"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "test_dequantize(peft_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsloth Harness"
      ],
      "metadata": {
        "id": "w0hSfb0qWe-I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5pUaSN3JcM"
      },
      "source": [
        "Write your Triton kernel below, and test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P9ThmhbT2GPi"
      },
      "outputs": [],
      "source": [
        "# from triton import jit\n",
        "# import triton\n",
        "# import triton.language as tl\n",
        "\n",
        "# @triton.jit\n",
        "# def _your_dequantize_nf4_kernel():\n",
        "#     ### TRITON CODE GOES HERE\n",
        "#     return\n",
        "\n",
        "# def _your_dequantize_nf4(weight, quant_state):\n",
        "#     ### SETUP TRITON LAUNCH HERE\n",
        "#     return None\n",
        "\n",
        "# def your_dequantize_nf4(weight):\n",
        "#     return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NF4 To FP16/BF16 Dequant in Mojo\n",
        "\n",
        "This notebook implements **bitsandbytes/Unsloth NF4 dequantization** as **pure Mojo GPU kernels**.\n",
        "\n",
        "The Goal:\n",
        "- Have a single fused kernel\n",
        "- Convert an NF4 quantized tensor to FP16/BF16\n",
        "- No large intermediate buffers\n",
        "- No `torch.compile` or Max graph magic in the case of Mojo\n",
        "- Must run on a T4\n",
        "- No C++ or CUDA\n",
        "- Do the full NF4 path including the â€œdouble dequantâ€ scale reconstruction\n",
        "- Correctness matched down to the small semantics details.\n",
        "\n",
        "## What this kernel does\n",
        "\n",
        "Given packed NF4 bytes (`uint8`) and the bnb quant state (`absmax_q`, `state2.code`, `state2.absmax`, `offset`), the kernel:\n",
        "\n",
        "* Decodes NF4 codes using the standard 16-value codebook\n",
        "* Reconstructs the per-block scale via `scale = state2.code[absmax_q] * state2.absmax[group] + offset`\n",
        "* Multiplies, casts, and stores\n",
        "\n",
        "### Exact bnb semantics\n",
        "\n",
        "This version matches bnb behavior exactly, including:\n",
        "\n",
        "* `blocksize = 64`, `state2.blocksize = 256`\n",
        "* **high nibble = first weight**, low nibble = second\n",
        "* separate **FP16** and **BF16** specialized kernels (no runtime dtype branching)\n",
        "\n",
        "## Benchmarking\n",
        "\n",
        "- Harness uses the same three `(hd, m)` configs as Unslothâ€™s `test_dequantize`\n",
        "- Dequantizes the same three matrices per config (`up`, `gate`, `down`).\n",
        "- Includes **mixed precision** (1Ã—FP16 + 2Ã—BF16) to mirror the original notebookâ€™s dtype schedule\n",
        "\n",
        "# Final Results Across Different GPUs\n",
        "\n",
        "All runs were done at the same time on the same runtime\n",
        "\n",
        "Totals for the dequant workload on each GPU.\n",
        "\n",
        "| GPU | Unsloth `fast_dequantize` | GB/s  | Mojo kernel | GB/s  | Speedup |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| T4 | 3.6984355449676514 | 162.6315 | 3.461897315 | 173.7435 | 1.068x |\n",
        "| L4 | 3.0031237602233887 | 200.2855 | 2.404278076 | 250.17166 | 1.249x |\n",
        "| A100 - 40Gb | 1.2074337005615234 | 498.1492 | 0.656471045 | 916.2357 | 1.839x |\n",
        "| H100 - PCIe | 0.6175928115844727 |  973.9139 | 0.408032219 | 1474.1047 | 1.514x |"
      ],
      "metadata": {
        "id": "UMyRVMkdl3ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Mojo"
      ],
      "metadata": {
        "id": "1G-8lh-rCeo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q mojo --index-url https://dl.modular.com/public/nightly/python/simple/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Y5MpAXh1UxZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052bcf08-5b43-4f1a-e244-534508224273"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.3/101.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m591.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.6/84.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check that Mojo is available and import notebook"
      ],
      "metadata": {
        "id": "FCCpcex8VYoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mojo.notebook"
      ],
      "metadata": {
        "id": "1Mq432ioVFkX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check that we are on the GPU"
      ],
      "metadata": {
        "id": "krpKVRjqVjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mojo\n",
        "\n",
        "from gpu.host import DeviceContext\n",
        "\n",
        "fn kernel():\n",
        "    print(\"Hello from the GPU\")\n",
        "\n",
        "def main():\n",
        "    with DeviceContext() as ctx:\n",
        "        ctx.enqueue_function_checked[kernel, kernel](grid_dim=1, block_dim=1)\n",
        "        ctx.synchronize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFzWDyc5VqgL",
        "outputId": "89904d3e-a82a-43fb-93f8-af9d256a2ee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the GPU\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mojo NF4 To F16/BF16 Kernels"
      ],
      "metadata": {
        "id": "sd5XhTOi_ZfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mojo\n",
        "from gpu import block_idx, thread_idx, barrier\n",
        "from gpu.host import DeviceContext\n",
        "from gpu.memory import AddressSpace\n",
        "from memory import UnsafePointer, stack_allocation, bitcast\n",
        "from math import fma, ceildiv\n",
        "\n",
        "#######################\n",
        "# Types and Constants #\n",
        "#######################\n",
        "\n",
        "comptime packed_dtype = DType.uint8\n",
        "comptime f32_dtype    = DType.float32\n",
        "comptime f16_dtype    = DType.float16\n",
        "comptime bf16_dtype   = DType.bfloat16\n",
        "comptime u32_dtype    = DType.uint32\n",
        "comptime u16_dtype    = DType.uint16\n",
        "\n",
        "comptime U8   = Scalar[packed_dtype]\n",
        "comptime F32  = Scalar[f32_dtype]\n",
        "comptime F16  = Scalar[f16_dtype]\n",
        "comptime BF16 = Scalar[bf16_dtype]\n",
        "comptime U32  = Scalar[u32_dtype]\n",
        "comptime U16  = Scalar[u16_dtype]\n",
        "\n",
        "# NF4 codebook (bitsandbytes)\n",
        "comptime NF4_TABLE = InlineArray[F32, 16](\n",
        "    -1.0,\n",
        "    -0.6961928009986877,\n",
        "    -0.5250730514526367,\n",
        "    -0.39491748809814453,\n",
        "    -0.28444138169288635,\n",
        "    -0.18477343022823334,\n",
        "    -0.09105003625154495,\n",
        "    0.0,\n",
        "    0.07958029955625534,\n",
        "    0.16093020141124725,\n",
        "    0.24611230194568634,\n",
        "    0.33791524171829224,\n",
        "    0.44070982933044434,\n",
        "    0.5626170039176941,\n",
        "    0.7229568362236023,\n",
        "    1.0\n",
        ")\n",
        "\n",
        "# constants (bnb NF4)\n",
        "comptime NF4_WEIGHTS_PER_BLOCK = 64   # 64 weights per NF4 block\n",
        "comptime NF4_BYTES_PER_BLOCK   = 32   # 64 / 2 two 4bit weights per byte\n",
        "comptime NF4_BLOCK_SHIFT       = 5    # log2(32) block_id = global_byte >> 5\n",
        "comptime STATE2_BLOCKS         = 256  # 256 NF4 blocks per state2 group\n",
        "comptime STATE2_SHIFT          = 8    # log2(256) group_id = block_id >> 8\n",
        "comptime CODE2_SIZE            = 256  # state2.code lookup table size\n",
        "\n",
        "# block config\n",
        "comptime TILE_BYTES_X = 256  #  bytes processed per row tile (x)\n",
        "comptime TILE_ROWS    = 4    #  rows processed per block     (y)\n",
        "comptime THREADS_X    = 128  #  threads along (x)\n",
        "comptime THREADS_Y    = 4    #  threads along (y)\n",
        "\n",
        "# Each thread handles 1 byte at   `byte = block_y * TILE_BYTES_X + tx`\n",
        "# plus an unrolled second byte at `byte + THREADS_X` -> 2 bytes / thread.\n",
        "\n",
        "# So per row:\n",
        "# 128 threads * 2 bytes/thread = 256 bytes\n",
        "\n",
        "#############################\n",
        "# Helpers: pack 2x16 -> u32 #\n",
        "#############################\n",
        "\n",
        "@always_inline\n",
        "fn pack2_u16_to_u32(lo16: U16, hi16: U16) -> U32:\n",
        "    \"\"\"Packs two 16-bit unsigned integers into a single 32-bit unsigned integer.\n",
        "\n",
        "    The low 16 bits contain the first value, and the high 16 bits contain the second value.\n",
        "\n",
        "    Parameters:\n",
        "        lo16: The lower 16-bit value\n",
        "        hi16: The higher 16-bit value\n",
        "\n",
        "    Returns:\n",
        "        A 32-bit unsigned integer with the packed values.\n",
        "    \"\"\"\n",
        "    return U32(lo16) | (U32(hi16) << 16)\n",
        "\n",
        "@always_inline\n",
        "fn pack2_f16_to_u32(a: F16, b: F16) -> U32:\n",
        "    \"\"\"Packs two 16-bit floating-point numbers (f16) into a single 32-bit unsigned integer.\n",
        "\n",
        "    Parameters:\n",
        "        a: The first f16 value\n",
        "        b: The second f16 value\n",
        "\n",
        "    Returns:\n",
        "        A 32-bit unsigned integer with the packed f16 values.\n",
        "    \"\"\"\n",
        "    var a_u16: U16 = bitcast[u16_dtype](a)\n",
        "    var b_u16: U16 = bitcast[u16_dtype](b)\n",
        "    return pack2_u16_to_u32(a_u16, b_u16)\n",
        "\n",
        "@always_inline\n",
        "fn pack2_bf16_to_u32(a: BF16, b: BF16) -> U32:\n",
        "    \"\"\"Packs two bfloat16 values into a single 32-bit unsigned integer.\n",
        "\n",
        "    Parameters:\n",
        "        a: The first bf16 value\n",
        "        b: The second bf16 value\n",
        "\n",
        "    Returns:\n",
        "        A 32-bit unsigned integer with the packed bf16 values\n",
        "    \"\"\"\n",
        "    var a_u16: U16 = bitcast[u16_dtype](a)\n",
        "    var b_u16: U16 = bitcast[u16_dtype](b)\n",
        "    return pack2_u16_to_u32(a_u16, b_u16)\n",
        "\n",
        "\n",
        "#####################################\n",
        "# GPU kernel: NF4 -> packed u32 (F16)#\n",
        "#####################################\n",
        "fn nf4_dequant_bnb_unroll2_packed_u32_f16(\n",
        "    packed_ptr:    UnsafePointer[U8,  MutAnyOrigin],  # W packed (nf4), length = n_rows * n_bytes_per_row\n",
        "    absmax_q_ptr:  UnsafePointer[U8,  MutAnyOrigin],  # quant_state.absmax (uint8), length = n_nf4_blocks\n",
        "    code2_ptr:     UnsafePointer[F32, MutAnyOrigin],  # quant_state.state2.code (float32), length = 256\n",
        "    absmax2_ptr:   UnsafePointer[F32, MutAnyOrigin],  # quant_state.state2.absmax (float32), length = n_groups\n",
        "    offset:        F32,                               # quant_state.offset\n",
        "    out_ptr:       UnsafePointer[U32, MutAnyOrigin],  # output: u32 per packed byte (2x f16)\n",
        "    n_rows:        Int,\n",
        "    n_bytes_per_row: Int,\n",
        "):\n",
        "    \"\"\"Dequantizes NF4 packed weights to packed u32 (containing two f16 values) on GPU with unrolling.\n",
        "\n",
        "    This kernel processes NF4 quantized weights, dequantizes them using the provided quantization states\n",
        "    Stores the results as packed u32 values (each holding two f16 floats).\n",
        "    Uses shared memory for the NF4 table and processes tiles of data with unrolling for efficiency.\n",
        "\n",
        "    Parameters:\n",
        "        packed_ptr: Pointer to packed NF4 weights (uint8)\n",
        "        absmax_q_ptr: Pointer to quantized absmax values (uint8)\n",
        "        code2_ptr: Pointer to state2 code table (float32)\n",
        "        absmax2_ptr: Pointer to state2 absmax values (float32)\n",
        "        offset: Quantization offset (float32)\n",
        "        out_ptr: Pointer to output packed u32 (each u32 holds two f16)\n",
        "        n_rows: Number of rows in the weight matrix.\n",
        "        n_bytes_per_row: Number of packed bytes per row\n",
        "\n",
        "    Note: Uses unrolling to process two bytes per thread for better performance.\n",
        "    \"\"\"\n",
        "    # Shared NF4 table\n",
        "    var sh_nf4 = stack_allocation[16, F32, address_space = AddressSpace.SHARED]()\n",
        "    var tx = Int(thread_idx.x)\n",
        "    var ty = Int(thread_idx.y)\n",
        "\n",
        "    if ty == 0 and tx < 16:\n",
        "        sh_nf4[tx] = NF4_TABLE[tx]\n",
        "    barrier()\n",
        "\n",
        "    var row  = Int(block_idx.x) * TILE_ROWS + ty\n",
        "    var byte = Int(block_idx.y) * TILE_BYTES_X + tx\n",
        "    if row >= n_rows or byte >= n_bytes_per_row:\n",
        "        return\n",
        "\n",
        "    var row_base = row * n_bytes_per_row\n",
        "    var gbyte0   = row_base + byte\n",
        "\n",
        "    ############################\n",
        "    # Byte 0 (bnb nibble order)#\n",
        "    ############################\n",
        "    var p0_u8: U8 = packed_ptr[gbyte0]\n",
        "    var p0: Int = Int(p0_u8)\n",
        "\n",
        "    # bnb semantics:\n",
        "    # first weight  = HIGH nibble\n",
        "    # second weight = LOW  nibble\n",
        "    var idx0_first:  Int = (p0 >> 4) & 0x0F\n",
        "    var idx0_second: Int =  p0       & 0x0F\n",
        "\n",
        "    var w0n: F32 = sh_nf4[idx0_first]\n",
        "    var w1n: F32 = sh_nf4[idx0_second]\n",
        "\n",
        "    var block0: Int = gbyte0 >> NF4_BLOCK_SHIFT\n",
        "    var q0: U8 = absmax_q_ptr[block0]\n",
        "    var code0: F32 = code2_ptr[Int(q0)]\n",
        "    var absmax20: F32 = absmax2_ptr[block0 >> STATE2_SHIFT]\n",
        "    var scale0: F32 = fma(code0, absmax20, offset)\n",
        "\n",
        "    var w0: F32 = w0n * scale0\n",
        "    var w1: F32 = w1n * scale0\n",
        "\n",
        "    var h0: F16 = F16(w0)\n",
        "    var h1: F16 = F16(w1)\n",
        "    out_ptr[gbyte0] = pack2_f16_to_u32(h0, h1)\n",
        "\n",
        "    ###############################\n",
        "    # Byte 1 (unroll +128 bytes)  #\n",
        "    ###############################\n",
        "    var byte1 = byte + THREADS_X\n",
        "    if byte1 < n_bytes_per_row:\n",
        "        var gbyte1 = gbyte0 + THREADS_X\n",
        "\n",
        "        var p1_u8: U8 = packed_ptr[gbyte1]\n",
        "        var p1: Int = Int(p1_u8)\n",
        "\n",
        "        var idx1_first:  Int = (p1 >> 4) & 0x0F\n",
        "        var idx1_second: Int =  p1       & 0x0F\n",
        "\n",
        "        var w2n: F32 = sh_nf4[idx1_first]\n",
        "        var w3n: F32 = sh_nf4[idx1_second]\n",
        "\n",
        "        # +128 bytes = +4 NF4 blocks (32 bytes/block)\n",
        "        var block1: Int = block0 + 4\n",
        "        var q1: U8 = absmax_q_ptr[block1]\n",
        "        var code1: F32 = code2_ptr[Int(q1)]\n",
        "        var absmax21: F32 = absmax2_ptr[block1 >> STATE2_SHIFT]\n",
        "        var scale1: F32 = fma(code1, absmax21, offset)\n",
        "\n",
        "        var w2: F32 = w2n * scale1\n",
        "        var w3: F32 = w3n * scale1\n",
        "\n",
        "        var h2: F16 = F16(w2)\n",
        "        var h3: F16 = F16(w3)\n",
        "        out_ptr[gbyte1] = pack2_f16_to_u32(h2, h3)\n",
        "\n",
        "########################################\n",
        "# GPU kernel: NF4 -> packed u32 (BF16) #\n",
        "########################################\n",
        "fn nf4_dequant_bnb_unroll2_packed_u32_bf16(\n",
        "    packed_ptr:    UnsafePointer[U8,  MutAnyOrigin],\n",
        "    absmax_q_ptr:  UnsafePointer[U8,  MutAnyOrigin],\n",
        "    code2_ptr:     UnsafePointer[F32, MutAnyOrigin],\n",
        "    absmax2_ptr:   UnsafePointer[F32, MutAnyOrigin],\n",
        "    offset:        F32,\n",
        "    out_ptr:       UnsafePointer[U32, MutAnyOrigin],\n",
        "    n_rows:        Int,\n",
        "    n_bytes_per_row: Int,\n",
        "):\n",
        "    \"\"\"Dequantizes NF4 packed weights to packed u32 (containing two bf16 values) on GPU with unrolling.\n",
        "\n",
        "    This kernel processes NF4 quantized weights, dequantizes them using the provided quantization states\n",
        "    Stores the results as packed u32 values (each holding two bf16 floats).\n",
        "    Uses shared memory for the NF4 table and processes tiles of data with unrolling for efficiency.\n",
        "\n",
        "    Parameters:\n",
        "        packed_ptr: Pointer to packed NF4 weights (uint8)\n",
        "        absmax_q_ptr: Pointer to quantized absmax values (uint8)\n",
        "        code2_ptr: Pointer to state2 code table (float32)\n",
        "        absmax2_ptr: Pointer to state2 absmax values (float32)\n",
        "        offset: Quantization offset (float32)\n",
        "        out_ptr: Pointer to output packed u32 (each u32 holds two bf16)\n",
        "        n_rows: Number of rows in the weight matrix.\n",
        "        n_bytes_per_row: Number of packed bytes per row.\n",
        "\n",
        "    Note: Uses unrolling to process two bytes per thread for better performance.\n",
        "    \"\"\"\n",
        "    var sh_nf4 = stack_allocation[16, F32, address_space = AddressSpace.SHARED]()\n",
        "    var tx = Int(thread_idx.x)\n",
        "    var ty = Int(thread_idx.y)\n",
        "\n",
        "    if ty == 0 and tx < 16:\n",
        "        sh_nf4[tx] = NF4_TABLE[tx]\n",
        "    barrier()\n",
        "\n",
        "    var row  = Int(block_idx.x) * TILE_ROWS + ty\n",
        "    var byte = Int(block_idx.y) * TILE_BYTES_X + tx\n",
        "    if row >= n_rows or byte >= n_bytes_per_row:\n",
        "        return\n",
        "\n",
        "    var row_base = row * n_bytes_per_row\n",
        "    var gbyte0   = row_base + byte\n",
        "\n",
        "    var p0_u8: U8 = packed_ptr[gbyte0]\n",
        "    var p0: Int = Int(p0_u8)\n",
        "\n",
        "    var idx0_first:  Int = (p0 >> 4) & 0x0F\n",
        "    var idx0_second: Int =  p0       & 0x0F\n",
        "\n",
        "    var w0n: F32 = sh_nf4[idx0_first]\n",
        "    var w1n: F32 = sh_nf4[idx0_second]\n",
        "\n",
        "    var block0: Int = gbyte0 >> NF4_BLOCK_SHIFT\n",
        "    var q0: U8 = absmax_q_ptr[block0]\n",
        "    var code0: F32 = code2_ptr[Int(q0)]\n",
        "    var absmax20: F32 = absmax2_ptr[block0 >> STATE2_SHIFT]\n",
        "    var scale0: F32 = fma(code0, absmax20, offset)\n",
        "\n",
        "    var w0: F32 = w0n * scale0\n",
        "    var w1: F32 = w1n * scale0\n",
        "\n",
        "    var b0: BF16 = BF16(w0)\n",
        "    var b1: BF16 = BF16(w1)\n",
        "    out_ptr[gbyte0] = pack2_bf16_to_u32(b0, b1)\n",
        "\n",
        "    var byte1 = byte + THREADS_X\n",
        "    if byte1 < n_bytes_per_row:\n",
        "        var gbyte1 = gbyte0 + THREADS_X\n",
        "\n",
        "        var p1_u8: U8 = packed_ptr[gbyte1]\n",
        "        var p1: Int = Int(p1_u8)\n",
        "\n",
        "        var idx1_first:  Int = (p1 >> 4) & 0x0F\n",
        "        var idx1_second: Int =  p1       & 0x0F\n",
        "\n",
        "        var w2n: F32 = sh_nf4[idx1_first]\n",
        "        var w3n: F32 = sh_nf4[idx1_second]\n",
        "\n",
        "        var block1: Int = block0 + 4\n",
        "        var q1: U8 = absmax_q_ptr[block1]\n",
        "        var code1: F32 = code2_ptr[Int(q1)]\n",
        "        var absmax21: F32 = absmax2_ptr[block1 >> STATE2_SHIFT]\n",
        "        var scale1: F32 = fma(code1, absmax21, offset)\n",
        "\n",
        "        var w2: F32 = w2n * scale1\n",
        "        var w3: F32 = w3n * scale1\n",
        "\n",
        "        var b2: BF16 = BF16(w2)\n",
        "        var b3: BF16 = BF16(w3)\n",
        "        out_ptr[gbyte1] = pack2_bf16_to_u32(b2, b3)\n",
        "\n",
        "#################\n",
        "# CPU reference #\n",
        "#################\n",
        "fn nf4_dequant_cpu_bnb_f16(\n",
        "    packed:   UnsafePointer[U8,  MutAnyOrigin],\n",
        "    absmax_q: UnsafePointer[U8,  MutAnyOrigin],\n",
        "    code2:    UnsafePointer[F32, MutAnyOrigin],\n",
        "    absmax2:  UnsafePointer[F32, MutAnyOrigin],\n",
        "    offset:   F32,\n",
        "    output:   UnsafePointer[F16, MutAnyOrigin],\n",
        "    n_rows:   Int,\n",
        "    n_cols:   Int,\n",
        "):\n",
        "    \"\"\"CPU reference implementation for dequantizing NF4 weights to f16.\n",
        "\n",
        "    This function dequantizes NF4 packed weights using the bitsandbytes (bnb) semantics\n",
        "    and stores the results as f16 values.\n",
        "\n",
        "    Parameters:\n",
        "        packed: Pointer to packed NF4 weights (uint8)\n",
        "        absmax_q: Pointer to quantized absmax values (uint8)\n",
        "        code2: Pointer to state2 code table (float32)\n",
        "        absmax2: Pointer to state2 absmax values (float32)\n",
        "        offset: Quantization offset (float32)\n",
        "        output: Pointer to output f16 weights\n",
        "        n_rows: Number of rows in the matrix\n",
        "        n_cols: Number of columns in the matrix\n",
        "    \"\"\"\n",
        "    var n_bytes_per_row = n_cols >> 1\n",
        "    for row in range(n_rows):\n",
        "        var row_byte_base = row * n_bytes_per_row\n",
        "        var row_out_base  = row * n_cols\n",
        "        for byte_idx in range(n_bytes_per_row):\n",
        "            var gbyte = row_byte_base + byte_idx\n",
        "            var pb: U8 = packed[gbyte]\n",
        "            var p: Int = Int(pb)\n",
        "\n",
        "            var idx_first:  Int = (p >> 4) & 0x0F\n",
        "            var idx_second: Int =  p       & 0x0F\n",
        "\n",
        "            var w0n: F32 = NF4_TABLE[idx_first]\n",
        "            var w1n: F32 = NF4_TABLE[idx_second]\n",
        "\n",
        "            var block: Int = gbyte >> NF4_BLOCK_SHIFT\n",
        "            var q: U8 = absmax_q[block]\n",
        "            var codev: F32 = code2[Int(q)]\n",
        "            var absmax2v: F32 = absmax2[block >> STATE2_SHIFT]\n",
        "            var scale: F32 = fma(codev, absmax2v, offset)\n",
        "\n",
        "            var col0 = byte_idx << 1\n",
        "            output[row_out_base + col0]     = F16(w0n * scale)\n",
        "            output[row_out_base + col0 + 1] = F16(w1n * scale)\n",
        "\n",
        "fn nf4_dequant_cpu_bnb_bf16(\n",
        "    packed:   UnsafePointer[U8,  MutAnyOrigin],\n",
        "    absmax_q: UnsafePointer[U8,  MutAnyOrigin],\n",
        "    code2:    UnsafePointer[F32, MutAnyOrigin],\n",
        "    absmax2:  UnsafePointer[F32, MutAnyOrigin],\n",
        "    offset:   F32,\n",
        "    output:   UnsafePointer[BF16, MutAnyOrigin],\n",
        "    n_rows:   Int,\n",
        "    n_cols:   Int,\n",
        "):\n",
        "    \"\"\"CPU reference implementation for dequantizing NF4 weights to bf16.\n",
        "\n",
        "    This function dequantizes NF4 packed weights using the bitsandbytes semantics\n",
        "    and stores the results as bf16 values.\n",
        "\n",
        "    Parameters:\n",
        "        packed: Pointer to packed NF4 weights (uint8)\n",
        "        absmax_q: Pointer to quantized absmax values (uint8)\n",
        "        code2: Pointer to state2 code table (float32)\n",
        "        absmax2: Pointer to state2 absmax values (float32)\n",
        "        offset: Quantization offset (float32)\n",
        "        output: Pointer to output bf16 weights\n",
        "        n_rows: Number of rows in the matrix\n",
        "        n_cols: Number of columns in the matrix\n",
        "    \"\"\"\n",
        "    var n_bytes_per_row = n_cols >> 1\n",
        "    for row in range(n_rows):\n",
        "        var row_byte_base = row * n_bytes_per_row\n",
        "        var row_out_base  = row * n_cols\n",
        "        for byte_idx in range(n_bytes_per_row):\n",
        "            var gbyte = row_byte_base + byte_idx\n",
        "            var pb: U8 = packed[gbyte]\n",
        "            var p: Int = Int(pb)\n",
        "\n",
        "            var idx_first:  Int = (p >> 4) & 0x0F\n",
        "            var idx_second: Int =  p       & 0x0F\n",
        "\n",
        "            var w0n: F32 = NF4_TABLE[idx_first]\n",
        "            var w1n: F32 = NF4_TABLE[idx_second]\n",
        "\n",
        "            var block: Int = gbyte >> NF4_BLOCK_SHIFT\n",
        "            var q: U8 = absmax_q[block]\n",
        "            var codev: F32 = code2[Int(q)]\n",
        "            var absmax2v: F32 = absmax2[block >> STATE2_SHIFT]\n",
        "            var scale: F32 = fma(codev, absmax2v, offset)\n",
        "\n",
        "            var col0 = byte_idx << 1\n",
        "            output[row_out_base + col0]     = BF16(w0n * scale)\n",
        "            output[row_out_base + col0 + 1] = BF16(w1n * scale)\n",
        "\n",
        "#################################################\n",
        "# Correctness tests (cross 256-block boundary)  #\n",
        "#################################################\n",
        "def run_correctness_test_f16(ctx: DeviceContext):\n",
        "    \"\"\"Runs correctness test for NF4 dequantization to f16.\n",
        "\n",
        "    This test compares GPU dequantization results with CPU reference implementation\n",
        "    for f16 output, checking for maximum absolute difference.\n",
        "\n",
        "    Parameters:\n",
        "        ctx: The device context for GPU operations.\n",
        "\n",
        "    Note: Tests across 256-block boundaries to ensure correctness in grouping.\n",
        "    \"\"\"\n",
        "    var n_rows: Int = 2\n",
        "    var n_cols: Int = 16384\n",
        "    var n_weights: Int = n_rows * n_cols\n",
        "    var n_bytes: Int = n_weights >> 1\n",
        "    var n_bytes_per_row: Int = n_cols >> 1\n",
        "\n",
        "    var n_nf4_blocks: Int = ceildiv(n_bytes, NF4_BYTES_PER_BLOCK)\n",
        "    var n_groups: Int = ceildiv(n_nf4_blocks, STATE2_BLOCKS)\n",
        "    if n_groups < 1: n_groups = 1\n",
        "\n",
        "    var offset: F32 = 0.01\n",
        "\n",
        "    var packed_host   = ctx.enqueue_create_host_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_host = ctx.enqueue_create_host_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_host    = ctx.enqueue_create_host_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_host  = ctx.enqueue_create_host_buffer[f32_dtype](n_groups)\n",
        "    var out_cpu_host  = ctx.enqueue_create_host_buffer[f16_dtype](n_weights)\n",
        "\n",
        "    ctx.synchronize()\n",
        "\n",
        "    # high nibble = first, low nibble = second\n",
        "    for i in range(n_bytes):\n",
        "        var first:  Int = (i + 1) & 15\n",
        "        var second: Int = i & 15\n",
        "        packed_host[i] = U8((first << 4) | second)\n",
        "\n",
        "    for i in range(n_nf4_blocks):\n",
        "        absmax_q_host[i] = U8(i & 255)\n",
        "\n",
        "    for i in range(CODE2_SIZE):\n",
        "        var x: Float64 = (Float64(i) - 127.5) / 127.5\n",
        "        code2_host[i] = F32(x)\n",
        "\n",
        "    for g in range(n_groups):\n",
        "        absmax2_host[g] = F32(1.0 + Float64(g))\n",
        "\n",
        "    nf4_dequant_cpu_bnb_f16(\n",
        "        packed_host.unsafe_ptr(),\n",
        "        absmax_q_host.unsafe_ptr(),\n",
        "        code2_host.unsafe_ptr(),\n",
        "        absmax2_host.unsafe_ptr(),\n",
        "        offset,\n",
        "        out_cpu_host.unsafe_ptr(),\n",
        "        n_rows,\n",
        "        n_cols,\n",
        "    )\n",
        "\n",
        "    var packed_dev   = ctx.enqueue_create_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_dev = ctx.enqueue_create_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_dev    = ctx.enqueue_create_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_dev  = ctx.enqueue_create_buffer[f32_dtype](n_groups)\n",
        "    var out_dev      = ctx.enqueue_create_buffer[u32_dtype](n_bytes)\n",
        "\n",
        "    ctx.enqueue_copy(dst_buf=packed_dev,   src_buf=packed_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax_q_dev, src_buf=absmax_q_host)\n",
        "    ctx.enqueue_copy(dst_buf=code2_dev,    src_buf=code2_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax2_dev,  src_buf=absmax2_host)\n",
        "\n",
        "    var grid_x: Int = ceildiv(n_rows, TILE_ROWS)\n",
        "    var grid_y: Int = ceildiv(n_bytes_per_row, TILE_BYTES_X)\n",
        "\n",
        "    var compiled = ctx.compile_function_checked[\n",
        "        nf4_dequant_bnb_unroll2_packed_u32_f16, nf4_dequant_bnb_unroll2_packed_u32_f16\n",
        "    ]()\n",
        "\n",
        "    @always_inline\n",
        "    @parameter\n",
        "    fn run_kernel(test_ctx: DeviceContext) raises:\n",
        "        test_ctx.enqueue_function_checked(\n",
        "            compiled,\n",
        "            packed_dev,\n",
        "            absmax_q_dev,\n",
        "            code2_dev,\n",
        "            absmax2_dev,\n",
        "            offset,\n",
        "            out_dev,\n",
        "            n_rows,\n",
        "            n_bytes_per_row,\n",
        "            grid_dim=(grid_x, grid_y),\n",
        "            block_dim=(THREADS_X, THREADS_Y),\n",
        "        )\n",
        "\n",
        "    run_kernel(ctx)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    with out_dev.map_to_host() as out_gpu_host:\n",
        "        var max_abs_diff: F32 = 0.0\n",
        "        for i in range(n_bytes):\n",
        "            var w: U32 = out_gpu_host[i]\n",
        "            var lo: U16 = U16(w & 0xFFFF)\n",
        "            var hi: U16 = U16((w >> 16) & 0xFFFF)\n",
        "            var g0: F16 = bitcast[f16_dtype](lo)\n",
        "            var g1: F16 = bitcast[f16_dtype](hi)\n",
        "\n",
        "            var idx0 = i << 1\n",
        "            var idx1 = idx0 + 1\n",
        "\n",
        "            var c0: F32 = F32(out_cpu_host[idx0])\n",
        "            var c1: F32 = F32(out_cpu_host[idx1])\n",
        "            var f0: F32 = F32(g0)\n",
        "            var f1: F32 = F32(g1)\n",
        "\n",
        "            var d0: F32 = c0 - f0\n",
        "            var d1: F32 = c1 - f1\n",
        "            if d0 < 0.0: d0 = -d0\n",
        "            if d1 < 0.0: d1 = -d1\n",
        "            if d0 > max_abs_diff: max_abs_diff = d0\n",
        "            if d1 > max_abs_diff: max_abs_diff = d1\n",
        "\n",
        "        print(\"[F16] max_abs_diff =\", max_abs_diff)\n",
        "        print(\"[F16] Correctness:\", \"PASSED\" if max_abs_diff < 1e-05 else \"FAILED\")\n",
        "\n",
        "def run_correctness_test_bf16(ctx: DeviceContext):\n",
        "    \"\"\"Runs correctness test for NF4 dequantization to bf16.\n",
        "\n",
        "    This test compares GPU dequantization results with CPU reference implementation\n",
        "    for bf16 output, checking for maximum absolute difference.\n",
        "\n",
        "    Parameters:\n",
        "        ctx: The device context for GPU operations\n",
        "\n",
        "    Note: Tests across 256-block boundaries to ensure correctness in grouping.\n",
        "    \"\"\"\n",
        "    var n_rows: Int = 2\n",
        "    var n_cols: Int = 16384\n",
        "    var n_weights: Int = n_rows * n_cols\n",
        "    var n_bytes: Int = n_weights >> 1\n",
        "    var n_bytes_per_row: Int = n_cols >> 1\n",
        "\n",
        "    var n_nf4_blocks: Int = ceildiv(n_bytes, NF4_BYTES_PER_BLOCK)\n",
        "    var n_groups: Int = ceildiv(n_nf4_blocks, STATE2_BLOCKS)\n",
        "    if n_groups < 1: n_groups = 1\n",
        "\n",
        "    var offset: F32 = 0.01\n",
        "\n",
        "    var packed_host   = ctx.enqueue_create_host_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_host = ctx.enqueue_create_host_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_host    = ctx.enqueue_create_host_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_host  = ctx.enqueue_create_host_buffer[f32_dtype](n_groups)\n",
        "    var out_cpu_host  = ctx.enqueue_create_host_buffer[bf16_dtype](n_weights)\n",
        "\n",
        "    ctx.synchronize()\n",
        "\n",
        "    for i in range(n_bytes):\n",
        "        var first:  Int = (i + 1) & 15\n",
        "        var second: Int = i & 15\n",
        "        packed_host[i] = U8((first << 4) | second)\n",
        "\n",
        "    for i in range(n_nf4_blocks):\n",
        "        absmax_q_host[i] = U8(i & 255)\n",
        "\n",
        "    for i in range(CODE2_SIZE):\n",
        "        var x: Float64 = (Float64(i) - 127.5) / 127.5\n",
        "        code2_host[i] = F32(x)\n",
        "\n",
        "    for g in range(n_groups):\n",
        "        absmax2_host[g] = F32(1.0 + Float64(g))\n",
        "\n",
        "    nf4_dequant_cpu_bnb_bf16(\n",
        "        packed_host.unsafe_ptr(),\n",
        "        absmax_q_host.unsafe_ptr(),\n",
        "        code2_host.unsafe_ptr(),\n",
        "        absmax2_host.unsafe_ptr(),\n",
        "        offset,\n",
        "        out_cpu_host.unsafe_ptr(),\n",
        "        n_rows,\n",
        "        n_cols,\n",
        "    )\n",
        "\n",
        "    var packed_dev   = ctx.enqueue_create_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_dev = ctx.enqueue_create_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_dev    = ctx.enqueue_create_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_dev  = ctx.enqueue_create_buffer[f32_dtype](n_groups)\n",
        "    var out_dev      = ctx.enqueue_create_buffer[u32_dtype](n_bytes)\n",
        "\n",
        "    ctx.enqueue_copy(dst_buf=packed_dev,   src_buf=packed_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax_q_dev, src_buf=absmax_q_host)\n",
        "    ctx.enqueue_copy(dst_buf=code2_dev,    src_buf=code2_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax2_dev,  src_buf=absmax2_host)\n",
        "\n",
        "    var grid_x: Int = ceildiv(n_rows, TILE_ROWS)\n",
        "    var grid_y: Int = ceildiv(n_bytes_per_row, TILE_BYTES_X)\n",
        "\n",
        "    var compiled = ctx.compile_function_checked[\n",
        "        nf4_dequant_bnb_unroll2_packed_u32_bf16, nf4_dequant_bnb_unroll2_packed_u32_bf16\n",
        "    ]()\n",
        "\n",
        "    @always_inline\n",
        "    @parameter\n",
        "    fn run_kernel(test_ctx: DeviceContext) raises:\n",
        "        test_ctx.enqueue_function_checked(\n",
        "            compiled,\n",
        "            packed_dev,\n",
        "            absmax_q_dev,\n",
        "            code2_dev,\n",
        "            absmax2_dev,\n",
        "            offset,\n",
        "            out_dev,\n",
        "            n_rows,\n",
        "            n_bytes_per_row,\n",
        "            grid_dim=(grid_x, grid_y),\n",
        "            block_dim=(THREADS_X, THREADS_Y),\n",
        "        )\n",
        "\n",
        "    run_kernel(ctx)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    with out_dev.map_to_host() as out_gpu_host:\n",
        "        var max_abs_diff: F32 = 0.0\n",
        "        for i in range(n_bytes):\n",
        "            var w: U32 = out_gpu_host[i]\n",
        "            var lo: U16 = U16(w & 0xFFFF)\n",
        "            var hi: U16 = U16((w >> 16) & 0xFFFF)\n",
        "            var g0: BF16 = bitcast[bf16_dtype](lo)\n",
        "            var g1: BF16 = bitcast[bf16_dtype](hi)\n",
        "\n",
        "            var idx0 = i << 1\n",
        "            var idx1 = idx0 + 1\n",
        "\n",
        "            var c0: F32 = F32(out_cpu_host[idx0])\n",
        "            var c1: F32 = F32(out_cpu_host[idx1])\n",
        "            var f0: F32 = F32(g0)\n",
        "            var f1: F32 = F32(g1)\n",
        "\n",
        "            var d0: F32 = c0 - f0\n",
        "            var d1: F32 = c1 - f1\n",
        "            if d0 < 0.0: d0 = -d0\n",
        "            if d1 < 0.0: d1 = -d1\n",
        "            if d0 > max_abs_diff: max_abs_diff = d0\n",
        "            if d1 > max_abs_diff: max_abs_diff = d1\n",
        "\n",
        "        print(\"[BF16] max_abs_diff =\", max_abs_diff)\n",
        "        print(\"[BF16] Correctness:\", \"PASSED\" if max_abs_diff < 1e-05 else \"FAILED\")\n",
        "\n",
        "###################################\n",
        "# Bench a single 2D weight matrix #\n",
        "####################################\n",
        "fn bench_layer_unroll2_f16(ctx: DeviceContext, n_rows: Int, n_cols: Int, num_iters: Int) raises -> Float64:\n",
        "    \"\"\"Benchmarks the NF4 dequantization kernel for f16 output on a single layer.\n",
        "\n",
        "    Measures the execution time for dequantizing a 2D weight matrix to packed u32 (f16) over multiple iterations.\n",
        "\n",
        "    Parameters:\n",
        "        ctx: The device context for GPU operations\n",
        "        n_rows: Number of rows in the matrix\n",
        "        n_cols: Number of columns in the matrix\n",
        "        num_iters: Number of iterations to run the benchmark\n",
        "\n",
        "    Returns:\n",
        "        Total execution time in seconds.\n",
        "\n",
        "    Note: Initializes test data with patterns to simulate real quantization states.\n",
        "    \"\"\"\n",
        "    var n_weights = n_rows * n_cols\n",
        "    var n_bytes = n_weights >> 1\n",
        "    var n_bytes_per_row = n_cols >> 1\n",
        "\n",
        "    var n_nf4_blocks: Int = ceildiv(n_bytes, NF4_BYTES_PER_BLOCK)\n",
        "    var n_groups: Int = ceildiv(n_nf4_blocks, STATE2_BLOCKS)\n",
        "    if n_groups < 1: n_groups = 1\n",
        "\n",
        "    var offset: F32 = 0.01\n",
        "\n",
        "    var packed_host   = ctx.enqueue_create_host_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_host = ctx.enqueue_create_host_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_host    = ctx.enqueue_create_host_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_host  = ctx.enqueue_create_host_buffer[f32_dtype](n_groups)\n",
        "\n",
        "    ctx.synchronize()\n",
        "\n",
        "    for i in range(n_bytes):\n",
        "        var first:  Int = (i + 1) & 15\n",
        "        var second: Int = i & 15\n",
        "        packed_host[i] = U8((first << 4) | second)\n",
        "\n",
        "    for i in range(n_nf4_blocks):\n",
        "        absmax_q_host[i] = U8(i & 255)\n",
        "\n",
        "    for i in range(CODE2_SIZE):\n",
        "        var x: Float64 = (Float64(i) - 127.5) / 127.5\n",
        "        code2_host[i] = F32(x)\n",
        "\n",
        "    for g in range(n_groups):\n",
        "        absmax2_host[g] = F32(1.5)\n",
        "\n",
        "    var packed_dev   = ctx.enqueue_create_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_dev = ctx.enqueue_create_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_dev    = ctx.enqueue_create_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_dev  = ctx.enqueue_create_buffer[f32_dtype](n_groups)\n",
        "    var out_dev      = ctx.enqueue_create_buffer[u32_dtype](n_bytes)\n",
        "\n",
        "    ctx.enqueue_copy(dst_buf=packed_dev,   src_buf=packed_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax_q_dev, src_buf=absmax_q_host)\n",
        "    ctx.enqueue_copy(dst_buf=code2_dev,    src_buf=code2_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax2_dev,  src_buf=absmax2_host)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    var grid_x: Int = ceildiv(n_rows, TILE_ROWS)\n",
        "    var grid_y: Int = ceildiv(n_bytes_per_row, TILE_BYTES_X)\n",
        "\n",
        "    var compiled = ctx.compile_function_checked[\n",
        "        nf4_dequant_bnb_unroll2_packed_u32_f16, nf4_dequant_bnb_unroll2_packed_u32_f16\n",
        "    ]()\n",
        "\n",
        "    @always_inline\n",
        "    @parameter\n",
        "    fn run_kernel(bench_ctx: DeviceContext) raises:\n",
        "        bench_ctx.enqueue_function_checked(\n",
        "            compiled,\n",
        "            packed_dev,\n",
        "            absmax_q_dev,\n",
        "            code2_dev,\n",
        "            absmax2_dev,\n",
        "            offset,\n",
        "            out_dev,\n",
        "            n_rows,\n",
        "            n_bytes_per_row,\n",
        "            grid_dim=(grid_x, grid_y),\n",
        "            block_dim=(THREADS_X, THREADS_Y),\n",
        "        )\n",
        "\n",
        "    run_kernel(ctx)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    var total_ns = ctx.execution_time[run_kernel](num_iters)\n",
        "    return Float64(total_ns) / 1e9\n",
        "\n",
        "fn bench_layer_unroll2_bf16(ctx: DeviceContext, n_rows: Int, n_cols: Int, num_iters: Int) raises -> Float64:\n",
        "    \"\"\"Benchmarks the NF4 dequantization kernel for bf16 output on a single layer.\n",
        "\n",
        "    Measures the execution time for dequantizing a 2D weight matrix to packed u32 (bf16) over multiple iterations.\n",
        "\n",
        "    Parameters:\n",
        "        ctx: The device context for GPU operations\n",
        "        n_rows: Number of rows in the matrix\n",
        "        n_cols: Number of columns in the matrix\n",
        "        num_iters: Number of iterations to run the benchmark\n",
        "\n",
        "    Returns:\n",
        "        Total execution time in seconds\n",
        "\n",
        "    Note: Initializes test data with patterns to simulate real quantization states\n",
        "    \"\"\"\n",
        "    var n_weights = n_rows * n_cols\n",
        "    var n_bytes = n_weights >> 1\n",
        "    var n_bytes_per_row = n_cols >> 1\n",
        "\n",
        "    var n_nf4_blocks: Int = ceildiv(n_bytes, NF4_BYTES_PER_BLOCK)\n",
        "    var n_groups: Int = ceildiv(n_nf4_blocks, STATE2_BLOCKS)\n",
        "    if n_groups < 1: n_groups = 1\n",
        "\n",
        "    var offset: F32 = 0.01\n",
        "\n",
        "    var packed_host   = ctx.enqueue_create_host_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_host = ctx.enqueue_create_host_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_host    = ctx.enqueue_create_host_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_host  = ctx.enqueue_create_host_buffer[f32_dtype](n_groups)\n",
        "\n",
        "    ctx.synchronize()\n",
        "\n",
        "    for i in range(n_bytes):\n",
        "        var first:  Int = (i + 1) & 15\n",
        "        var second: Int = i & 15\n",
        "        packed_host[i] = U8((first << 4) | second)\n",
        "\n",
        "    for i in range(n_nf4_blocks):\n",
        "        absmax_q_host[i] = U8(i & 255)\n",
        "\n",
        "    for i in range(CODE2_SIZE):\n",
        "        var x: Float64 = (Float64(i) - 127.5) / 127.5\n",
        "        code2_host[i] = F32(x)\n",
        "\n",
        "    for g in range(n_groups):\n",
        "        absmax2_host[g] = F32(1.5)\n",
        "\n",
        "    var packed_dev   = ctx.enqueue_create_buffer[packed_dtype](n_bytes)\n",
        "    var absmax_q_dev = ctx.enqueue_create_buffer[packed_dtype](n_nf4_blocks)\n",
        "    var code2_dev    = ctx.enqueue_create_buffer[f32_dtype](CODE2_SIZE)\n",
        "    var absmax2_dev  = ctx.enqueue_create_buffer[f32_dtype](n_groups)\n",
        "    var out_dev      = ctx.enqueue_create_buffer[u32_dtype](n_bytes)\n",
        "\n",
        "    ctx.enqueue_copy(dst_buf=packed_dev,   src_buf=packed_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax_q_dev, src_buf=absmax_q_host)\n",
        "    ctx.enqueue_copy(dst_buf=code2_dev,    src_buf=code2_host)\n",
        "    ctx.enqueue_copy(dst_buf=absmax2_dev,  src_buf=absmax2_host)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    var grid_x: Int = ceildiv(n_rows, TILE_ROWS)\n",
        "    var grid_y: Int = ceildiv(n_bytes_per_row, TILE_BYTES_X)\n",
        "\n",
        "    var compiled = ctx.compile_function_checked[\n",
        "        nf4_dequant_bnb_unroll2_packed_u32_bf16, nf4_dequant_bnb_unroll2_packed_u32_bf16\n",
        "    ]()\n",
        "\n",
        "    @always_inline\n",
        "    @parameter\n",
        "    fn run_kernel(bench_ctx: DeviceContext) raises:\n",
        "        bench_ctx.enqueue_function_checked(\n",
        "            compiled,\n",
        "            packed_dev,\n",
        "            absmax_q_dev,\n",
        "            code2_dev,\n",
        "            absmax2_dev,\n",
        "            offset,\n",
        "            out_dev,\n",
        "            n_rows,\n",
        "            n_bytes_per_row,\n",
        "            grid_dim=(grid_x, grid_y),\n",
        "            block_dim=(THREADS_X, THREADS_Y),\n",
        "        )\n",
        "\n",
        "    run_kernel(ctx)\n",
        "    ctx.synchronize()\n",
        "\n",
        "    var total_ns = ctx.execution_time[run_kernel](num_iters)\n",
        "    return Float64(total_ns) / 1e9\n",
        "\n",
        "\n",
        "########################\n",
        "# Unsloth-like Harness #\n",
        "########################\n",
        "def main():\n",
        "    \"\"\"Main entry point for running NF4 dequantization benchmarks and correctness tests.\n",
        "\n",
        "    This function sets up the device context, runs correctness tests for f16 and bf16,\n",
        "    and performs benchmarks for mixed precision configurations similar to Unsloth tests.\n",
        "    \"\"\"\n",
        "    with DeviceContext() as ctx:\n",
        "        print(\"=\" * 43)\n",
        "        print(\"NF4 Dequant (bnb-correct) | Mixed Precision\")\n",
        "        print(\"=\" * 43)\n",
        "        print(\"Tile:\", TILE_BYTES_X, \"bytes/row  | Rows/tile:\", TILE_ROWS)\n",
        "        print(\"Block:\", THREADS_X, \"x\", THREADS_Y, \"=\", THREADS_X * THREADS_Y, \"threads\")\n",
        "        print(\"Each thread: 2 bytes via unroll2\")\n",
        "        print(\"2 u32 stores (packed 2x16-bit floats)\")\n",
        "        print(\"Device:\", ctx.api())\n",
        "\n",
        "        print(\"\\n--- Correctness (F16) ---\")\n",
        "        run_correctness_test_f16(ctx)\n",
        "\n",
        "        print(\"\\n--- Correctness (BF16) ---\")\n",
        "        run_correctness_test_bf16(ctx)\n",
        "\n",
        "        var NUM_ITERS: Int = 1000\n",
        "\n",
        "        # Unsloth test_dequantize options:\n",
        "        # (bsz, qlen, hd, m, seed, dtype)\n",
        "        # (2, 3333, 2048,  8192, 3407, fp16)\n",
        "        # (5,  777, 1024,  4096, 3409, bf16)\n",
        "        # (3, 2048, 4096, 14336, 3408, bf16)\n",
        "        var hd1: Int = 2048\n",
        "        var m1:  Int = 8192\n",
        "        var hd2: Int = 1024\n",
        "        var m2:  Int = 4096\n",
        "        var hd3: Int = 4096\n",
        "        var m3:  Int = 14336\n",
        "\n",
        "        # MLP weight shapes (bnb Linear4bit):\n",
        "        # up_proj   : (m,  hd)\n",
        "        # gate_proj : (m,  hd)\n",
        "        # down_proj : (hd, m)\n",
        "\n",
        "        @always_inline\n",
        "        fn bench_config_f16(ctx: DeviceContext, hd: Int, m: Int, iters: Int) raises -> Float64:\n",
        "            var t_up   = bench_layer_unroll2_f16(ctx, m,  hd, iters)\n",
        "            var t_gate = bench_layer_unroll2_f16(ctx, m,  hd, iters)\n",
        "            var t_down = bench_layer_unroll2_f16(ctx, hd, m,  iters)\n",
        "            return t_up + t_gate + t_down\n",
        "\n",
        "        @always_inline\n",
        "        fn bench_config_bf16(ctx: DeviceContext, hd: Int, m: Int, iters: Int) raises -> Float64:\n",
        "            var t_up   = bench_layer_unroll2_bf16(ctx, m,  hd, iters)\n",
        "            var t_gate = bench_layer_unroll2_bf16(ctx, m,  hd, iters)\n",
        "            var t_down = bench_layer_unroll2_bf16(ctx, hd, m,  iters)\n",
        "            return t_up + t_gate + t_down\n",
        "\n",
        "        #########################\n",
        "        # Mixed FP16 x1 BF16 x2 #\n",
        "        #########################\n",
        "        print(\"\\n\" + \"-\" * 35)\n",
        "        print(\"MIXED - FP16 x1, BF16 x2\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        var total_mixed: Float64 = 0.0\n",
        "\n",
        "        print(\"Config 1 [FP16]: hd=\", hd1, \" m=\", m1)\n",
        "        var c1_mixed = bench_config_f16(ctx, hd1, m1, NUM_ITERS)\n",
        "        print(\"  total:\", c1_mixed, \"s\")\n",
        "        total_mixed += c1_mixed\n",
        "\n",
        "        print(\"Config 2 [BF16]: hd=\", hd2, \" m=\", m2)\n",
        "        var c2_mixed = bench_config_bf16(ctx, hd2, m2, NUM_ITERS)\n",
        "        print(\"  total:\", c2_mixed, \"s\")\n",
        "        total_mixed += c2_mixed\n",
        "\n",
        "        print(\"Config 3 [BF16]: hd=\", hd3, \" m=\", m3)\n",
        "        var c3_mixed = bench_config_bf16(ctx, hd3, m3, NUM_ITERS)\n",
        "        print(\"  total:\", c3_mixed, \"s\\n\")\n",
        "        total_mixed += c3_mixed\n",
        "\n",
        "        print(\"MIXED TOTAL:\", total_mixed, \"s\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 35)\n",
        "        print(\"  MIXED:\", total_mixed, \"s\")\n",
        "        print(\"=\" * 35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD7mxL-B7CV6",
        "outputId": "89946cfe-df32-45bb-b05e-327c3f3a8eeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================\n",
            "NF4 Dequant (bnb-correct) | Mixed Precision\n",
            "===========================================\n",
            "Tile: 256 bytes/row  | Rows/tile: 4\n",
            "Block: 128 x 4 = 512 threads\n",
            "Each thread: 2 bytes via unroll2\n",
            "2 u32 stores (packed 2x16-bit floats)\n",
            "Device: cuda\n",
            "\n",
            "--- Correctness (F16) ---\n",
            "[F16] max_abs_diff = 0.0\n",
            "[F16] Correctness: PASSED\n",
            "\n",
            "--- Correctness (BF16) ---\n",
            "[BF16] max_abs_diff = 0.0\n",
            "[BF16] Correctness: PASSED\n",
            "\n",
            "-----------------------------------\n",
            "MIXED - FP16 x1, BF16 x2\n",
            "-----------------------------------\n",
            "Config 1 [FP16]: hd= 2048  m= 8192\n",
            "  total: 0.7430141889999999 s\n",
            "Config 2 [BF16]: hd= 1024  m= 4096\n",
            "  total: 0.172033949 s\n",
            "Config 3 [BF16]: hd= 4096  m= 14336\n",
            "  total: 2.484810362 s\n",
            "\n",
            "MIXED TOTAL: 3.3998585 s\n",
            "\n",
            "===================================\n",
            "  MIXED: 3.3998585 s\n",
            "===================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate The Effective GB/s"
      ],
      "metadata": {
        "id": "ao1YwcHBJkEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iters = 1000\n",
        "layers = 3\n",
        "configs = [\n",
        "    (2048, 8192),\n",
        "    (1024, 4096),\n",
        "    (4096, 14336),\n",
        "]\n",
        "total_weights = layers * iters * sum(r*c for r, c in configs)\n",
        "\n",
        "# Traffic model (bytes moved per dequantized FP16 value)\n",
        "# - output fp16: 2 bytes\n",
        "# - packed nf4: 0.5 bytes (1 byte -> 2 weights)\n",
        "# - absmax_q: 1 byte per 64 weights\n",
        "# - absmax2: 4 bytes per (64 weights * 256 blocks) = 16384 weights\n",
        "bytes_per_weight = 2.0 + 0.5 + (1/64) + (4/(64*256))\n",
        "\n",
        "def effective_gbps(seconds: float, bytes_per_weight: float = bytes_per_weight) -> float:\n",
        "    return (total_weights * bytes_per_weight) / (seconds * 1e9)\n",
        "\n",
        "times_unsloth = {\n",
        "    \"T4\": 3.6984355449676514,\n",
        "    \"\\nL4\": 3.0031237602233887,\n",
        "    \"\\nA100 40GB\": 1.2074337005615234,\n",
        "    \"\\nH100 PCIe\": 0.6175928115844727\n",
        "    }\n",
        "times_mojo    = {\n",
        "    \"T4\": 3.461897315,\n",
        "    \"\\nL4\": 2.404278076,\n",
        "    \"\\nA100 40GB\": 0.656471045,\n",
        "    \"\\nH100 PCIe\": 0.408032219\n",
        "    }\n",
        "\n",
        "for gpu in [\"T4\",\"\\nL4\",\"\\nA100 40GB\",\"\\nH100 PCIe\"]:\n",
        "    print(gpu, \"\\nunsloth GB/s\", effective_gbps(times_unsloth[gpu]),\n",
        "               \"\\nmojo    GB/s\",    effective_gbps(times_mojo[gpu]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EedcU2YGMgP",
        "outputId": "283f2ae0-c379-4fd6-9f42-0dc00903fe4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T4 \n",
            "unsloth GB/s 162.63153235654426 \n",
            "mojo    GB/s 173.7435242211972\n",
            "\n",
            "L4 \n",
            "unsloth GB/s 200.28553200726515 \n",
            "mojo    GB/s 250.1716610920009\n",
            "\n",
            "A100 40GB \n",
            "unsloth GB/s 498.1492894560401 \n",
            "mojo    GB/s 916.2357495904484\n",
            "\n",
            "H100 PCIe \n",
            "unsloth GB/s 973.9139263244662 \n",
            "mojo    GB/s 1474.104769162849\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "w0hSfb0qWe-I"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}